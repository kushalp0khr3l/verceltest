{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-03T11:57:51.474739Z",
     "iopub.status.busy": "2026-02-03T11:57:51.474399Z",
     "iopub.status.idle": "2026-02-03T11:57:52.048743Z",
     "shell.execute_reply": "2026-02-03T11:57:52.047908Z",
     "shell.execute_reply.started": "2026-02-03T11:57:51.474704Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-03T11:57:52.051000Z",
     "iopub.status.busy": "2026-02-03T11:57:52.050699Z",
     "iopub.status.idle": "2026-02-03T11:57:56.113634Z",
     "shell.execute_reply": "2026-02-03T11:57:56.112862Z",
     "shell.execute_reply.started": "2026-02-03T11:57:52.050965Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -U \\\n",
    "  transformers \\\n",
    "  peft \\\n",
    "  accelerate \\\n",
    "  bitsandbytes \\\n",
    "  trl \\\n",
    "  datasets \\\n",
    "  huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-03T11:57:56.114915Z",
     "iopub.status.busy": "2026-02-03T11:57:56.114691Z",
     "iopub.status.idle": "2026-02-03T11:57:56.119715Z",
     "shell.execute_reply": "2026-02-03T11:57:56.118996Z",
     "shell.execute_reply.started": "2026-02-03T11:57:56.114888Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "HF_TOKEN = \"token yeta rakha\"\n",
    "\n",
    "os.environ[\"HF_TOKEN\"] = HF_TOKEN\n",
    "os.environ[\"HUGGINGFACE_HUB_TOKEN\"] = HF_TOKEN\n",
    "os.environ[\"TRANSFORMERS_TOKEN\"] = HF_TOKEN\n",
    "\n",
    "print(\"HF_TOKEN set:\", HF_TOKEN[:8] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-03T11:57:56.122164Z",
     "iopub.status.busy": "2026-02-03T11:57:56.121740Z",
     "iopub.status.idle": "2026-02-03T11:58:00.372165Z",
     "shell.execute_reply": "2026-02-03T11:58:00.371456Z",
     "shell.execute_reply.started": "2026-02-03T11:57:56.122119Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoConfig\n",
    "\n",
    "model_id = \"unsloth/gemma-3-12b-it-bnb-4bit\"\n",
    "\n",
    "# Tokenizer test\n",
    "tok = AutoTokenizer.from_pretrained(model_id)\n",
    "print(\"Tokenizer loaded\")\n",
    "\n",
    "# Config test\n",
    "cfg = AutoConfig.from_pretrained(model_id)\n",
    "print(\"Config loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-03T11:58:00.373177Z",
     "iopub.status.busy": "2026-02-03T11:58:00.372969Z",
     "iopub.status.idle": "2026-02-03T11:58:00.379629Z",
     "shell.execute_reply": "2026-02-03T11:58:00.378881Z",
     "shell.execute_reply.started": "2026-02-03T11:58:00.373157Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%writefile train_lora.py\n",
    "import argparse\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Args\n",
    "# -----------------------------\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--model_name_or_path\", type=str, required=True)\n",
    "    parser.add_argument(\"--dataset_path\", type=str, required=True)\n",
    "    parser.add_argument(\"--output_dir\", type=str, required=True)\n",
    "    parser.add_argument(\"--max_seq_length\", type=int, default=1024)\n",
    "    return parser.parse_args()\n",
    "\n",
    "# -----------------------------\n",
    "# Main\n",
    "# -----------------------------\n",
    "def main():\n",
    "    args = parse_args()\n",
    "\n",
    "    # -----------------------------\n",
    "    # Tokenizer\n",
    "    # -----------------------------\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        args.model_name_or_path,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "\n",
    "    # -----------------------------\n",
    "    # Model (already 4-bit)\n",
    "    # -----------------------------\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        args.model_name_or_path,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "\n",
    "    model.config.use_cache = False\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    model.gradient_checkpointing_enable()\n",
    "\n",
    "    # -----------------------------\n",
    "    # LoRA\n",
    "    # -----------------------------\n",
    "    lora_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.05,\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "    # -----------------------------\n",
    "    # Dataset\n",
    "    # -----------------------------\n",
    "    raw_ds = load_dataset(\n",
    "        \"json\",\n",
    "        data_files=args.dataset_path,\n",
    "        split=\"train\",\n",
    "    )\n",
    "\n",
    "    def format_and_tokenize(example):\n",
    "        instruction = example[\"instruction\"]\n",
    "        user_input = example[\"input\"]\n",
    "        output = example[\"output\"]\n",
    "    \n",
    "        prompt = (\n",
    "            f\"{instruction}\\n\\n\"\n",
    "            f\"{user_input}\\n\\n\"\n",
    "            \"Answer:\\n\"\n",
    "        )\n",
    "    \n",
    "        prompt_ids = tokenizer(\n",
    "            prompt,\n",
    "            truncation=True,\n",
    "            max_length=args.max_seq_length,\n",
    "            add_special_tokens=False,\n",
    "        )[\"input_ids\"]\n",
    "    \n",
    "        answer_ids = tokenizer(\n",
    "            output,\n",
    "            truncation=True,\n",
    "            max_length=args.max_seq_length - len(prompt_ids),\n",
    "            add_special_tokens=False,\n",
    "        )[\"input_ids\"]\n",
    "    \n",
    "        input_ids = prompt_ids + answer_ids\n",
    "        labels = [-100] * len(prompt_ids) + answer_ids\n",
    "    \n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"labels\": labels,\n",
    "            \"attention_mask\": [1] * len(input_ids),\n",
    "            # REQUIRED FOR GEMMA-3\n",
    "            \"token_type_ids\": [0] * len(input_ids),\n",
    "        }\n",
    "\n",
    "\n",
    "    dataset = raw_ds.map(\n",
    "        format_and_tokenize,\n",
    "        remove_columns=raw_ds.column_names,\n",
    "        desc=\"Tokenizing dataset\",\n",
    "    )\n",
    "\n",
    "    # -----------------------------\n",
    "    # Training\n",
    "    # -----------------------------\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=args.output_dir,\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=16,\n",
    "        num_train_epochs=2,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=True,\n",
    "        logging_steps=5,\n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit=1,\n",
    "        report_to=\"none\",\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    model.save_pretrained(args.output_dir)\n",
    "    tokenizer.save_pretrained(args.output_dir)\n",
    "\n",
    "    print(\"LoRA training finished correctly\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-03T11:58:00.547474Z",
     "iopub.status.busy": "2026-02-03T11:58:00.546772Z",
     "iopub.status.idle": "2026-02-03T11:58:00.700865Z",
     "shell.execute_reply": "2026-02-03T11:58:00.700238Z",
     "shell.execute_reply.started": "2026-02-03T11:58:00.547442Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!ls /kaggle/input/askm-processed-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-03T11:58:00.702461Z",
     "iopub.status.busy": "2026-02-03T11:58:00.702216Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!accelerate launch \\\n",
    "  --num_processes=1 \\\n",
    "  train_lora.py \\\n",
    "  --model_name_or_path unsloth/gemma-3-12b-it-bnb-4bit \\\n",
    "  --dataset_path /kaggle/input/askm-processed-datasets/exam_lora.jsonl \\\n",
    "  --output_dir /kaggle/working/lora_outputs/exam_lora \\\n",
    "  --max_seq_length 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!ls /kaggle/working/lora_outputs/exam_lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!zip -r exam_lora.zip /kaggle/working/lora_outputs/exam_lora"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 9390547,
     "sourceId": 14699445,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31260,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
